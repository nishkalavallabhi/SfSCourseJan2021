{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Snorkel Tutorial: Data Labeling and Augmentation for Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will walk through the process of using Snorkel to build a training set for classifying YouTube comments as spam or not spam.\n",
    "The goal of this tutorial is to illustrate the basic components and concepts of Snorkel in a simple way, but also to dive into the actual process of iteratively developing real applications in Snorkel.\n",
    "\n",
    "**Note that this is a toy dataset that helps highlight the different features of Snorkel. For examples of high-performance, real-world uses of Snorkel, see [publications list](https://www.snorkel.org/resources/).**\n",
    "\n",
    "Our goal is to train a classifier over the comment data that can predict whether a comment is spam or not spam.\n",
    "We have access to a large amount of *unlabeled data* in the form of YouTube comments with some metadata.\n",
    "In order to train a classifier, we need to label our data, but doing so by hand for real world applications can often be prohibitively slow and expensive.\n",
    "\n",
    "In these cases, we can turn to a _weak supervision_ approach, using **_labeling functions (LFs)_** in Snorkel: noisy, programmatic rules and heuristics that assign labels to unlabeled training data.\n",
    "We'll dive into the Snorkel API and how we write labeling functions later in this tutorial, but as an example,\n",
    "we can write an LF that labels data points with `\"http\"` in the comment text as spam since many spam\n",
    "comments contain links:\n",
    "\n",
    "```python\n",
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "@labeling_function()\n",
    "def lf_contains_link(x):\n",
    "    # Return a label of SPAM if \"http\" in comment text, otherwise ABSTAIN\n",
    "    return SPAM if \"http\" in x.text.lower() else ABSTAIN\n",
    "```\n",
    "\n",
    "The tutorial is divided into four parts:\n",
    "1. **Loading Data**: We load a [YouTube comments dataset](http://www.dt.fee.unicamp.br/~tiago//youtubespamcollection/), originally introduced in [\"TubeSpam: Comment Spam Filtering on YouTube\"](https://ieeexplore.ieee.org/document/7424299/), ICMLA'15 (T.C. Alberto, J.V. Lochter, J.V. Almeida).\n",
    "\n",
    "2. **Writing Labeling Functions**: We write Python programs that take as input a data point and assign labels (or abstain) using heuristics, pattern matching, and third-party models.\n",
    "\n",
    "3. **Combining Labeling Function Outputs with the Label Model**: We model the outputs of the labeling functions over the training set using a novel, theoretically-grounded [modeling approach](https://arxiv.org/abs/1605.07723), which estimates the accuracies and correlations of the labeling functions using only their agreements and disagreements, and then uses this to reweight and combine their outputs, which we then use as _probabilistic_ training labels.\n",
    "\n",
    "4. **Training a Classifier**: We train a classifier that can predict labels for *any* YouTube comment (not just the ones labeled by the labeling functions) using the probabilistic training labels from step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Spam Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a [YouTube comments dataset](http://www.dt.fee.unicamp.br/~tiago//youtubespamcollection/) that consists of YouTube comments from 5 videos. The task is to classify each comment as being\n",
    "\n",
    "* **`HAM`**: comments relevant to the video (even very simple ones), or\n",
    "* **`SPAM`**: irrelevant (often trying to advertise something) or inappropriate messages\n",
    "\n",
    "For example, the following comments are `SPAM`:\n",
    "\n",
    "        \"Subscribe to me for free Android games, apps..\"\n",
    "\n",
    "        \"Please check out my vidios\"\n",
    "\n",
    "        \"Subscribe to me and I'll subscribe back!!!\"\n",
    "\n",
    "and these are `HAM`:\n",
    "\n",
    "        \"3:46 so cute!\"\n",
    "\n",
    "        \"This looks so fun and it's a good song\"\n",
    "\n",
    "        \"This is a weird video.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splits in Snorkel\n",
    "\n",
    "We split our data into two sets:\n",
    "* **Training Set**: The largest split of the dataset, and the one without any ground truth (\"gold\") labels.\n",
    "We will generate labels for these data points with weak supervision.\n",
    "* **Test Set**: A small, standard held-out blind hand-labeled set for final evaluation of our classifier. This set should only be used for final evaluation, _not_ error analysis.\n",
    "\n",
    "Note that in more advanced production settings, we will often further split up the available hand-labeled data into a _development split_, for getting ideas to write labeling functions, and a _validation split_ for e.g. checking our performance without looking at test set scores, hyperparameter tuning, etc.  These splits are used in some of the other advanced tutorials, but omitted for simplicity here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the YouTube comments dataset and create Pandas DataFrame objects for the train and test sets.\n",
    "DataFrames are extremely popular in Python data analysis workloads, and Snorkel provides native support\n",
    "for several DataFrame-like data structures, including Pandas, Dask, and PySpark.\n",
    "For more information on working with Pandas DataFrames, see the [Pandas DataFrame guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html).\n",
    "\n",
    "Each DataFrame consists of the following fields:\n",
    "* **`author`**: Username of the comment author\n",
    "* **`data`**: Date and time the comment was posted\n",
    "* **`text`**: Raw text content of the comment\n",
    "* **`label`**: Whether the comment is `SPAM` (1), `HAM` (0), or `UNKNOWN/ABSTAIN` (-1)\n",
    "* **`video`**: Video the comment is associated with\n",
    "\n",
    "We start by loading our data.\n",
    "The `load_spam_dataset()` method downloads the raw CSV files from the internet, divides them into splits, converts them into DataFrames, and shuffles them.\n",
    "As mentioned above, the dataset contains comments from 5 of the most popular YouTube videos during a period between 2014 and 2015.\n",
    "* The first four videos' comments are combined to form the `train` set. This set has no gold labels.\n",
    "* The fifth video is part of the `test` set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "source": [
    "This next cell makes sure a spaCy English model is downloaded.\n",
    "If this is your first time downloading this model, restart the kernel after executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.3.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Download the spaCy english model\n",
    "! python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1586, 5)\n",
      "(250, 5)\n",
      "{0, 1}\n",
      "{-1.0}\n"
     ]
    }
   ],
   "source": [
    "#Downloading the dataset and loading it into the coding environment.\n",
    "from utils import load_spam_dataset\n",
    "\n",
    "df_train, df_test = load_spam_dataset()\n",
    "\n",
    "#TODO: How big s the dataset? \n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "# We pull out the label vectors for ease of use later\n",
    "Y_test = df_test.label.values\n",
    "Y_train = df_train.label.values\n",
    "print(set(Y_test))\n",
    "print(set(Y_train)) #everything is -1 here, as we don't want to use these labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class distribution varies slightly between `SPAM` and `HAM`, but they're approximately class-balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity, we define constants to represent the class labels for spam, ham, and abstaining.\n",
    "ABSTAIN = -1\n",
    "HAM = 0\n",
    "SPAM = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing Labeling Functions (LFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Exploring the training set for initial ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by looking at 20 random data points from the `train` set to generate some ideas for LFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>video</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Santeri Saariokari</td>\n",
       "      <td>Hey guys go to check my video name \"growtopia my story\"﻿</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>Lil M</td>\n",
       "      <td>Hi I&amp;#39;m lil m !!! Check out love the way you lie!!!! My live performance and many others,,, videos and my own lyrics!!!!! Thanks</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Tuấn Anh Nguyễn Tiến</td>\n",
       "      <td>1000000000 views.﻿</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>Adela Korman</td>\n",
       "      <td>Check out my SEXY VIDEO :*</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>andrewregino1</td>\n",
       "      <td>Check out this video on YouTube:﻿</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>ItsJoey Dash</td>\n",
       "      <td>EVERYONE PLEASE SUBSCRIBE TO MY CHANNEL OR CAN YOU ALL JUST GO LOOK AT MY VIDEOS ﻿</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>Evin Alshamas</td>\n",
       "      <td>Subscribe me please﻿</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>awsome models</td>\n",
       "      <td>plese subscribe to me﻿</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Cléda Dimitri</td>\n",
       "      <td>http://www.rtbf.be/tv/emission/detail_the-voice-belgique/toutes-les-auditions/auditionDetail_?emissionId=3873&amp;amp;id=342  Please join me to the voice Liked and shared it please to win more audition  score. Thanks so much﻿</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Julia Szefer</td>\n",
       "      <td>Katty is the best! ! ! ! ﻿</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Tiona Mayo</td>\n",
       "      <td>Anyone else think this video theme is a bit of an insult to 28 days later? ﻿</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>katy perry</td>\n",
       "      <td>Hey Katycats! We are releasing a movie at midnight UK time to celebrate  katy's 30th birthday! Be sure to subscribe to our channel and watch the  movie!﻿</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Patricia  Reyes</td>\n",
       "      <td>I love the song roar it make me think am fill the way﻿</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ahmed soliman</td>\n",
       "      <td>we all love you Katy Perry &amp;lt;3﻿</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Charlotte “Cais” Blackstone</td>\n",
       "      <td>Who df is Lauren Bennett..﻿</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Itss Synco</td>\n",
       "      <td>COME AND CHECK OUT MY NEW YOUTUBE CHHANEL, GOING TO BE POSTING DAILY!﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Stacy</td>\n",
       "      <td>check out my playlist</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Eduarda Ketrony</td>\n",
       "      <td>https://www.facebook.com/photo.php?fbid=543627485763966&amp;amp;l=0d878a889c﻿</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Lukas Ogonowski</td>\n",
       "      <td>2015!! LLIKEE!!﻿</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>Jack ToadROXMK</td>\n",
       "      <td>Anybody who subscribes to me will get 10 subscribers﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          author  \\\n",
       "99   Santeri Saariokari            \n",
       "359  Lil M                         \n",
       "154  Tuấn Anh Nguyễn Tiến          \n",
       "428  Adela Korman                  \n",
       "219  andrewregino1                 \n",
       "386  ItsJoey Dash                  \n",
       "266  Evin Alshamas                 \n",
       "39   awsome models                 \n",
       "16   Cléda Dimitri                 \n",
       "227  Julia Szefer                  \n",
       "166  Tiona Mayo                    \n",
       "78   katy perry                    \n",
       "297  Patricia  Reyes               \n",
       "53   ahmed soliman                 \n",
       "89   Charlotte “Cais” Blackstone   \n",
       "125  Itss Synco                    \n",
       "379  Stacy                         \n",
       "68   Eduarda Ketrony               \n",
       "173  Lukas Ogonowski               \n",
       "348  Jack ToadROXMK                \n",
       "\n",
       "                                                                                                                                                                                                                              text  \\\n",
       "99   Hey guys go to check my video name \"growtopia my story\"﻿                                                                                                                                                                        \n",
       "359  Hi I&#39;m lil m !!! Check out love the way you lie!!!! My live performance and many others,,, videos and my own lyrics!!!!! Thanks                                                                                             \n",
       "154  1000000000 views.﻿                                                                                                                                                                                                              \n",
       "428    Check out my SEXY VIDEO :*                                                                                                                                                                                                    \n",
       "219  Check out this video on YouTube:﻿                                                                                                                                                                                               \n",
       "386  EVERYONE PLEASE SUBSCRIBE TO MY CHANNEL OR CAN YOU ALL JUST GO LOOK AT MY VIDEOS ﻿                                                                                                                                              \n",
       "266  Subscribe me please﻿                                                                                                                                                                                                            \n",
       "39   plese subscribe to me﻿                                                                                                                                                                                                          \n",
       "16   http://www.rtbf.be/tv/emission/detail_the-voice-belgique/toutes-les-auditions/auditionDetail_?emissionId=3873&amp;id=342  Please join me to the voice Liked and shared it please to win more audition  score. Thanks so much﻿   \n",
       "227  Katty is the best! ! ! ! ﻿                                                                                                                                                                                                      \n",
       "166  Anyone else think this video theme is a bit of an insult to 28 days later? ﻿                                                                                                                                                    \n",
       "78   Hey Katycats! We are releasing a movie at midnight UK time to celebrate  katy's 30th birthday! Be sure to subscribe to our channel and watch the  movie!﻿                                                                       \n",
       "297  I love the song roar it make me think am fill the way﻿                                                                                                                                                                          \n",
       "53   we all love you Katy Perry &lt;3﻿                                                                                                                                                                                               \n",
       "89   Who df is Lauren Bennett..﻿                                                                                                                                                                                                     \n",
       "125  COME AND CHECK OUT MY NEW YOUTUBE CHHANEL, GOING TO BE POSTING DAILY!﻿                                                                                                                                                          \n",
       "379  check out my playlist                                                                                                                                                                                                           \n",
       "68   https://www.facebook.com/photo.php?fbid=543627485763966&amp;l=0d878a889c﻿                                                                                                                                                       \n",
       "173  2015!! LLIKEE!!﻿                                                                                                                                                                                                                \n",
       "348  Anybody who subscribes to me will get 10 subscribers﻿                                                                                                                                                                           \n",
       "\n",
       "     video  label  \n",
       "99   2     -1.0    \n",
       "359  4     -1.0    \n",
       "154  3     -1.0    \n",
       "428  4     -1.0    \n",
       "219  4     -1.0    \n",
       "386  3     -1.0    \n",
       "266  2     -1.0    \n",
       "39   4     -1.0    \n",
       "16   2     -1.0    \n",
       "227  2     -1.0    \n",
       "166  3     -1.0    \n",
       "78   2     -1.0    \n",
       "297  2     -1.0    \n",
       "53   2     -1.0    \n",
       "89   3     -1.0    \n",
       "125  1     -1.0    \n",
       "379  4     -1.0    \n",
       "68   2     -1.0    \n",
       "173  3     -1.0    \n",
       "348  1     -1.0    "
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[[\"author\", \"text\", \"video\", \"label\"]].sample(20, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One dominant pattern in the comments that look like spam (which we might know from prior domain experience, or from inspection of a few training data points) is the use of the phrase \"check out\" (e.g. \"check out my channel\").\n",
    "Let's start with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Writing a few LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling functions in Snorkel are created with the\n",
    "[`@labeling_function` decorator](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.labeling_function.html).\n",
    "The [decorator](https://realpython.com/primer-on-python-decorators/) can be applied to _any Python function_ that returns a label for a single data point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import re\n",
    "\n",
    "@labeling_function()\n",
    "def subscribe(x):\n",
    "    return SPAM if \"subscribe\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def help_me(x):\n",
    "    return SPAM if \"help\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def need_money(x):\n",
    "    return SPAM if \"money\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def http(x):\n",
    "    return SPAM if \"http\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def good(x):\n",
    "    return HAM if \"good\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def love(x):\n",
    "    return HAM if \"love\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def best(x):\n",
    "    return HAM if \"best\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def check_out(x):\n",
    "    return SPAM if re.search(r\"check.*out\", x.text, flags=re.I) else ABSTAIN\n",
    "\n",
    "\n",
    "from snorkel.preprocess import preprocessor\n",
    "from textblob import TextBlob\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def textblob_sentiment(x):\n",
    "    scores = TextBlob(x.text)\n",
    "    x.polarity = scores.sentiment.polarity\n",
    "    x.subjectivity = scores.sentiment.subjectivity\n",
    "    return x\n",
    "\n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_subjectivity(x):\n",
    "    return HAM if x.subjectivity >= 0.5 else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def short_comment(x):\n",
    "    \"\"\"Ham comments are often short, such as 'cool video!'\"\"\"\n",
    "    return HAM if len(x.text.split()) < 5 else ABSTAIN\n",
    "\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "# The SpacyPreprocessor parses the text in text_field and\n",
    "# stores the new enriched representation in doc_field\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)\n",
    "\n",
    "@labeling_function(pre=[spacy])\n",
    "def has_person(x):\n",
    "    \"\"\"Ham comments mention specific people and are short.\"\"\"\n",
    "    if len(x.doc) < 20 and any([ent.label_ == \"PERSON\" for ent in x.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "from snorkel.labeling.lf.nlp import nlp_labeling_function\n",
    "\n",
    "\n",
    "@nlp_labeling_function()\n",
    "def has_person_nlp(x):\n",
    "    \"\"\"Ham comments mention specific people and are short.\"\"\"\n",
    "    if len(x.doc) < 20 and any([ent.label_ == \"PERSON\" for ent in x.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply one or more LFs that we've written to a collection of data points, we use an\n",
    "[`LFApplier`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.LFApplier.html).\n",
    "Because our data points are represented with a Pandas DataFrame in this tutorial, we use the\n",
    "[`PandasLFApplier`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.PandasLFApplier.html).\n",
    "Correspondingly, a single data point `x` that's passed into our LFs will be a [Pandas `Series` object](https://pandas.pydata.org/pandas-docs/stable/reference/series.html).\n",
    "\n",
    "It's important to note that these LFs will work for any object with an attribute named `text`, not just Pandas objects.\n",
    "Snorkel has several other appliers for different data point collection types which you can browse in the [API documentation](https://snorkel.readthedocs.io/en/master/packages/labeling.html).\n",
    "\n",
    "The output of the `apply(...)` method is a ***label matrix***, a fundamental concept in Snorkel.\n",
    "It's a NumPy array `L` with one column for each LF and one row for each data point, where `L[i, j]` is the label that the `j`th labeling function output for the `i`th data point.\n",
    "We'll create a label matrix for the `train` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1586/1586 [00:01<00:00, 935.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "lfs = [subscribe, help_me, need_money, http, love, good, best, check_out,\n",
    "       textblob_subjectivity, short_comment, has_person_nlp]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Evaluate performance on training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of statistics about labeling functions &mdash; like coverage &mdash; are useful when building any Snorkel application.\n",
    "So Snorkel provides tooling for common LF analyses using the\n",
    "[`LFAnalysis` utility](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.LFAnalysis.html).\n",
    "We report the following summary statistics for multiple LFs at once:\n",
    "\n",
    "* **Polarity**: The set of unique labels this LF outputs (excluding abstains)\n",
    "* **Coverage**: The fraction of the dataset the LF labels\n",
    "* **Overlaps**: The fraction of the dataset where this LF and at least one other LF label\n",
    "* **Conflicts**: The fraction of the dataset where this LF and at least one other LF label and disagree\n",
    "* **Correct**: The number of data points this LF labels correctly (if gold labels are provided)\n",
    "* **Incorrect**: The number of data points this LF labels incorrectly (if gold labels are provided)\n",
    "* **Empirical Accuracy**: The empirical accuracy of this LF (if gold labels are provided)\n",
    "\n",
    "For *Correct*, *Incorrect*, and *Empirical Accuracy*, we don't want to penalize the LF for data points where it abstained.\n",
    "We calculate these statistics only over those data points where the LF output a label.\n",
    "**Note that in our current setup, we can't compute these statistics because we don't have any ground-truth labels (other than in the test set, which we cannot look at). Not to worry—Snorkel's `LabelModel` will estimate them without needing any ground-truth labels in the next step!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subscribe</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.127364</td>\n",
       "      <td>0.090164</td>\n",
       "      <td>0.072509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help_me</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.029634</td>\n",
       "      <td>0.026482</td>\n",
       "      <td>0.013241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need_money</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.028373</td>\n",
       "      <td>0.026482</td>\n",
       "      <td>0.022068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http</th>\n",
       "      <td>3</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.119168</td>\n",
       "      <td>0.092686</td>\n",
       "      <td>0.080706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.094578</td>\n",
       "      <td>0.088903</td>\n",
       "      <td>0.027112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>5</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.034048</td>\n",
       "      <td>0.029634</td>\n",
       "      <td>0.008827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>6</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.032156</td>\n",
       "      <td>0.024590</td>\n",
       "      <td>0.006305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>check_out</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.233922</td>\n",
       "      <td>0.098991</td>\n",
       "      <td>0.086381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <td>8</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.357503</td>\n",
       "      <td>0.244641</td>\n",
       "      <td>0.131778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_comment</th>\n",
       "      <td>9</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.225725</td>\n",
       "      <td>0.138714</td>\n",
       "      <td>0.066835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_person_nlp</th>\n",
       "      <td>10</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.071879</td>\n",
       "      <td>0.053594</td>\n",
       "      <td>0.025221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        j Polarity  Coverage  Overlaps  Conflicts\n",
       "subscribe              0   [1]      0.127364  0.090164  0.072509 \n",
       "help_me                1   [1]      0.029634  0.026482  0.013241 \n",
       "need_money             2   [1]      0.028373  0.026482  0.022068 \n",
       "http                   3   [1]      0.119168  0.092686  0.080706 \n",
       "love                   4   [0]      0.094578  0.088903  0.027112 \n",
       "good                   5   [0]      0.034048  0.029634  0.008827 \n",
       "best                   6   [0]      0.032156  0.024590  0.006305 \n",
       "check_out              7   [1]      0.233922  0.098991  0.086381 \n",
       "textblob_subjectivity  8   [0]      0.357503  0.244641  0.131778 \n",
       "short_comment          9   [0]      0.225725  0.138714  0.066835 \n",
       "has_person_nlp         10  [0]      0.071879  0.053594  0.025221 "
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "lfs = [subscribe, help_me, need_money, http, love, good, best, check_out,\n",
    "       textblob_subjectivity, short_comment, has_person_nlp]\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combining Labeling Function Outputs with the Label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates just a handful of the types of LFs that one might write for this task.\n",
    "One of the key goals of Snorkel is _not_ to replace the effort, creativity, and subject matter expertise required to come up with these labeling functions, but rather to make it faster to write them, since **in Snorkel the labeling functions are assumed to be noisy, i.e. innaccurate, overlapping, etc.**\n",
    "Said another way: the LF abstraction provides a flexible interface for conveying a huge variety of supervision signals, and the `LabelModel` is able to denoise these signals, reducing the need for painstaking manual fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we perform some LFs analysis and finalize our list,  we can now apply these once again with `LFApplier` to get the label matrices.\n",
    "The Pandas format provides an easy interface that many practitioners are familiar with, but it is also less optimized for scale.\n",
    "For larger datasets, more compute-intensive LFs, or larger LF sets, you may decide to use one of the other data formats\n",
    "that Snorkel supports natively, such as Dask DataFrames or PySpark DataFrames, and their corresponding applier objects.\n",
    "For more info, check out the [Snorkel API documentation](https://snorkel.readthedocs.io/en/master/packages/labeling.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1586/1586 [00:00<00:00, 7411.31it/s]\n",
      "100%|██████████| 250/250 [00:00<00:00, 909.51it/s]\n"
     ]
    }
   ],
   "source": [
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_test = applier.apply(df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subscribe</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.127364</td>\n",
       "      <td>0.090164</td>\n",
       "      <td>0.072509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help_me</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.029634</td>\n",
       "      <td>0.026482</td>\n",
       "      <td>0.013241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need_money</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.028373</td>\n",
       "      <td>0.026482</td>\n",
       "      <td>0.022068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http</th>\n",
       "      <td>3</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.119168</td>\n",
       "      <td>0.092686</td>\n",
       "      <td>0.080706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.094578</td>\n",
       "      <td>0.088903</td>\n",
       "      <td>0.027112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>5</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.034048</td>\n",
       "      <td>0.029634</td>\n",
       "      <td>0.008827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>6</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.032156</td>\n",
       "      <td>0.024590</td>\n",
       "      <td>0.006305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>check_out</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.233922</td>\n",
       "      <td>0.098991</td>\n",
       "      <td>0.086381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <td>8</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.357503</td>\n",
       "      <td>0.244641</td>\n",
       "      <td>0.131778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_comment</th>\n",
       "      <td>9</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.225725</td>\n",
       "      <td>0.138714</td>\n",
       "      <td>0.066835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_person_nlp</th>\n",
       "      <td>10</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.071879</td>\n",
       "      <td>0.053594</td>\n",
       "      <td>0.025221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        j Polarity  Coverage  Overlaps  Conflicts\n",
       "subscribe              0   [1]      0.127364  0.090164  0.072509 \n",
       "help_me                1   [1]      0.029634  0.026482  0.013241 \n",
       "need_money             2   [1]      0.028373  0.026482  0.022068 \n",
       "http                   3   [1]      0.119168  0.092686  0.080706 \n",
       "love                   4   [0]      0.094578  0.088903  0.027112 \n",
       "good                   5   [0]      0.034048  0.029634  0.008827 \n",
       "best                   6   [0]      0.032156  0.024590  0.006305 \n",
       "check_out              7   [1]      0.233922  0.098991  0.086381 \n",
       "textblob_subjectivity  8   [0]      0.357503  0.244641  0.131778 \n",
       "short_comment          9   [0]      0.225725  0.138714  0.066835 \n",
       "has_person_nlp         10  [0]      0.071879  0.053594  0.025221 "
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "source": [
    "We see that our labeling functions vary in coverage, how much they overlap/conflict with one another, and almost certainly their accuracies as well.\n",
    "We can view a histogram of how many LF labels the data points in our train set have to get an idea of our total coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAUx0lEQVR4nO3df7RdZX3n8fcnAfyFoJZMqwEMYNBGR0QjpbVLUFFhoYnLgoVii5Yp1YoiVqdpp0O7cGYN6Mi0U5GKqMWWQhFoyZQodQTUskAT0IpAgRgihEFJlV9iFUK+88fZ13W45N7sJHef4737/Vrrrnv2j7P3d99knc/Z+9n7eVJVSJL6a964C5AkjZdBIEk9ZxBIUs8ZBJLUcwaBJPXcTuMuYFvtsccetWjRonGXIUmzyvXXX/9vVbVgS8tmXRAsWrSINWvWjLsMSZpVknxnqmVeGpKknjMIJKnnDAJJ6jmDQJJ6ziCQpJ4zCCSp5wwCSeo5g0CSes4gkKSem3VPFs9Wi1ZcPpb9rj/9yLHsV9Ls4RmBJPWcQSBJPWcQSFLPGQSS1HMGgST1nEEgST1nEEhSzxkEktRzBoEk9ZxBIEk9ZxBIUs8ZBJLUcwaBJPWcQSBJPddpECQ5PMmtSdYmWTHNer+WpJIs7bIeSdITdRYESeYDZwFHAEuAY5Ms2cJ6TwdOBr7aVS2SpKl1eUZwELC2qtZV1SPAhcDyLaz3QeAM4Mcd1iJJmkKXQbAQuGtoekMz76eSvBTYq6qmHb4ryYlJ1iRZs3HjxpmvVJJ6bGyNxUnmAWcCv7+1davqnKpaWlVLFyxY0H1xktQjXQbB3cBeQ9N7NvMmPB14EXB1kvXAwcBKG4wlabS6DILVwOIk+yTZBTgGWDmxsKoeqKo9qmpRVS0CrgOWVdWaDmuSJE3SWRBU1SbgJOAK4Bbgoqq6KclpSZZ1tV9J0rbZqcuNV9UqYNWkeadOse6hXdYiSdoynyyWpJ4zCCSp5wwCSeo5g0CSes4gkKSeMwgkqecMAknqOYNAknrOIJCknjMIJKnnDAJJ6jmDQJJ6ziCQpJ4zCCSp5wwCSeo5g0CSes4gkKSeMwgkqecMAknqOYNAknrOIJCknjMIJKnnDAJJ6jmDQJJ6ziCQpJ4zCCSp5wwCSeo5g0CSem6rQZDkSW3mSZJmpzZnBNe2nCdJmoV2mmpBkl8AFgJPSXIgkGbRbsBTR1CbJGkEpgwC4PXA24A9gTOH5j8I/FGHNUmSRmjKIKiq84DzkvxaVV0ywpokSSPUpo3gmiSfTPI5gCRLkpzQcV2SpBFpEwSfBq4AntNM3wa8t7OKJEkj1SYI9qiqi4DNAFW1CXis06okSSPTJggeTvJzQAEkORh4oNOqJEkjM91dQxPeB6wE9ktyDbAAOKrTqiRJI7PVM4KqugE4BPgV4HeBF1bVN9tsPMnhSW5NsjbJii0sf0eSG5N8I8k/J1myrQcgSdoxbbqYOBp4SlXdBLwJ+LskL23xvvnAWcARwBLg2C180P9tVf3HqnoJ8CEe/7yCJGkE2rQR/NeqeijJrwKvAT4JnN3ifQcBa6tqXVU9AlwILB9eoaoeHJp8Gk07hCRpdNoEwcQdQkcCn6iqy4FdWrxvIXDX0PSGZt7jJHlXkm8zOCN4z5Y2lOTEJGuSrNm4cWOLXUuS2moTBHcn+Tjw68CqpufRGeu+uqrOqqr9gD8A/niKdc6pqqVVtXTBggUztWtJEu0+0N/C4IGy11fV/cCzgA+0eN/dwF5D03s286ZyIYM2CEnSCLW5a+hHVXUp8ECSvYGdgX9tse3VwOIk+yTZBTiGwW2oP5Vk8dDkkcDtrSuXJM2IrT5HkGQZ8BEGXUzcC+zNIAheON37qmpTkpMYnE3MBz5VVTclOQ1YU1UrgZOSHAY8CtwHHL8jByNJ2nZtHij7IHAw8H+r6sAkrwLe2mbjVbUKWDVp3qlDr0/ehlolSR1o00bwaFV9H5iXZF5VXQUs7bguSdKItDkjuD/JrsCXgfOT3As83G1ZkqRRaXNGsBz4EXAK8Hng28AbuixKkjQ6bYLg1KraXFWbquq8qvrfDO75lyTNAW2C4LVbmHfETBciSRqPKdsIkrwT+D1g3yTDvY0+Hbim68I0MxatuHxs+15/+pFj27ek9qZrLP5b4HPA/wCGu5B+qKp+0GlVkqSRmTIIquoBBiORHQuQ5D8ATwZ2TbJrVd05mhIlSV1qMx7BG5PcDtwBfAlYz+BMQZI0B7RpLP5vDJ4svq2q9mEwJsF1nVYlSRoZnyyWpJ7zyWJJ6rm2Txb/O49/sviNXRYlSRqdrZ4RVNXwt//zOqxFkjQG0z1Q9hDTDCZfVbt1UpEkaaSme47g6QBJPgjcA/w1EOA44NkjqU6S1Lk2bQTLqupjVfVQVT1YVWczaDeQJM0BbYLg4STHJZmfZF6S4/CuIUmaM9oEwW8AbwG+1/wc3cyTJM0Bbe4aWo+XgiRpzmpzRiBJmsMMAknquSmDIMnJze9XjK4cSdKoTXdG8Pbm91+MohBJ0nhM11h8SzMOwXMmDVUZoKrqxd2WJkkahemeLD42yS8AVwDLRleSJGmUpr19tKq+CxyQZBdg/2b2rVX1aOeVSZJGYqvPESQ5BPgMgyEqA+yV5Piq+nLHtUmSRqDNwDRnAq+rqlsBkuwPXAC8rMvCJEmj0eY5gp0nQgCgqm4Ddu6uJEnSKLU5I1iT5Fzgb5rp44A13ZUkSRqlNkHwTuBdwHua6a8AH+usIknSSLXpdO4nDNoJzuy+HEnSqNnXkCT1nEEgST1nEEhSz7V5oGx/4APAc4fXr6pXd1iXJGlE2tw19FngL4FPAI91W44kadTaBMGmqjp7ezae5HDgz4H5wLlVdfqk5e8D/hOwCdgI/HZVfWd79iVJ2j5t2gj+T5LfS/LsJM+a+Nnam5LMB84CjgCWAMcmWTJpta8DS5surS8GPrSN9UuSdlCbM4Ljm98fGJpXwL5bed9BwNqqWgeQ5EJgOXDzTzdSddXQ+tcBb21RjyRpBrV5oGyf7dz2QuCuoekNwC9Ns/4JwOe2c1+SpO3U5q6hnRl0M/HKZtbVwMdnckyCJG8FlgKHTLH8ROBEgL333numditJol0bwdkMupz+WPPzsmbe1twN7DU0vWcz73GSHAb8F2BZ053FE1TVOVW1tKqWLliwoMWuJUlttWkjeHlVHTA0fWWSf2nxvtXA4iT7MAiAY4DfGF4hyYHAx4HDq+reljVLkmZQmzOCx5LsNzGRZF9aPE9QVZuAkxiMeXwLcFFV3ZTktCQTYyB/GNgV+GySbyRZuc1HIEnaIW3OCD4AXJVkHYOhKp8LvL3NxqtqFbBq0rxTh14f1r5USVIX2tw19MUki4HnN7NunepaviRp9pkyCJK8uqquTPLmSYuel4SqurTj2iRJIzDdGcEhwJXAG7ewrACDQJLmgCmDoKr+pHl5WlXdMbysuRNIkjQHtLlr6JItzLt4pguRJI3HdG0ELwBeCOw+qZ1gN+DJXRcmSRqN6doIng+8AXgGj28neAj4nS6LkiSNznRtBJcBlyX55aq6doQ1SZJGqE0bwTuSPGNiIskzk3yqw5okSSPUJgheXFX3T0xU1X3Agd2VJEkapTZdTMxL8swmAGhGJ2vzvp85i1ZcPu4SJOlnTpsP9I8A1yb5LIO+ho4C/nunVUmSRqZNX0OfSXI98Kpm1pur6ubp3iNJmj1aXeJpuo/eSPP8QJK9q+rOTiuTJI3EVhuLkyxLcjtwB/AlYD2OLSxJc0abu4Y+CBwM3NYMZP8a4LpOq5IkjUybIHi0qr7P4O6heVV1FYOB5iVJc0CbNoL7k+wKfBk4P8m9wMPdliVJGpU2ZwTLgR8BpwCfB77NlscokCTNQtOeESSZD/xjVb0K2AycN5KqJEkjM+0ZQVU9BmxOsvuI6pEkjVibNoIfAjcm+QJDbQNV9Z7OqpIkjUybILgUxyeWpDlruhHK9q6qO6vKdgFJmsOmayP4h4kXSbY0brEkaQ6YLggy9HrfrguRJI3HdEFQU7yWJM0h0zUWH5DkQQZnBk9pXtNMV1Xt1nl1kqTOTTd4/fxRFiJJGo82XUxIkuYwg0CSes4gkKSeMwgkqecMAknqOYNAknrOIJCknjMIJKnnDAJJ6jmDQJJ6rtMgSHJ4kluTrE2yYgvLX5nkhiSbkhzVZS2SpC1rM0LZdmkGvj8LeC2wAVidZGVV3Ty02p3A24D3d1WHxmfRisvHst/1px85lv1Ks1VnQQAcBKytqnUASS4ElgM/DYKqWt8s29xhHZKkaXR5aWghcNfQ9IZm3jZLcmKSNUnWbNy4cUaKkyQNzIrG4qo6p6qWVtXSBQsWjLscSZpTugyCu4G9hqb3bOZJkn6GdBkEq4HFSfZJsgtwDLCyw/1JkrZDZ0FQVZuAk4ArgFuAi6rqpiSnJVkGkOTlSTYARwMfT3JTV/VIkrasy7uGqKpVwKpJ804der2awSUjSdKYzIrGYklSdwwCSeo5g0CSes4gkKSeMwgkqecMAknqOYNAknrOIJCknjMIJKnnDAJJ6jmDQJJ6ziCQpJ4zCCSp5wwCSeo5g0CSes4gkKSeMwgkqecMAknqOYNAknrOIJCknjMIJKnnDAJJ6jmDQJJ6ziCQpJ4zCCSp5wwCSeo5g0CSes4gkKSeMwgkqecMAknqOYNAknrOIJCknjMIJKnnDAJJ6jmDQJJ6ziCQpJ7badwFSDNt0YrLx7Lf9acfOZb9SjvKMwJJ6rlOgyDJ4UluTbI2yYotLH9Skr9rln81yaIu65EkPVFnQZBkPnAWcASwBDg2yZJJq50A3FdVzwP+F3BGV/VIkrasyzaCg4C1VbUOIMmFwHLg5qF1lgN/2ry+GPhoklRVdViX1IlxtU2A7RPaMV0GwULgrqHpDcAvTbVOVW1K8gDwc8C/Da+U5ETgxGbyh0lu3c6a9pi87R7wmHsgZ/TvmOnhvzM7dszPnWrBrLhrqKrOAc7Z0e0kWVNVS2egpFnDY+4Hj7kfujrmLhuL7wb2Gpres5m3xXWS7ATsDny/w5okSZN0GQSrgcVJ9kmyC3AMsHLSOiuB45vXRwFX2j4gSaPV2aWh5pr/ScAVwHzgU1V1U5LTgDVVtRL4JPDXSdYCP2AQFl3a4ctLs5DH3A8ecz90cszxC7gk9ZtPFktSzxkEktRzvQmCrXV3Mdck2SvJVUluTnJTkpPHXdMoJJmf5OtJ/nHctYxCkmckuTjJvya5Jckvj7umriU5pfk//a0kFyR58rhrmmlJPpXk3iTfGpr3rCRfSHJ78/uZM7W/XgRBy+4u5ppNwO9X1RLgYOBdPThmgJOBW8ZdxAj9OfD5qnoBcABz/NiTLATeAyytqhcxuBGl65tMxuGvgMMnzVsBfLGqFgNfbKZnRC+CgKHuLqrqEWCiu4s5q6ruqaobmtcPMfiAWDjeqrqVZE/gSODccdcyCkl2B17J4O47quqRqrp/vFWNxE7AU5pnj54K/L8x1zPjqurLDO6kHLYcOK95fR7wppnaX1+CYEvdXczpD8VhTa+uBwJfHW8lnfsz4D8Dm8ddyIjsA2wEPt1cDjs3ydPGXVSXqupu4H8CdwL3AA9U1T+Nt6qR+fmquqd5/V3g52dqw30Jgt5KsitwCfDeqnpw3PV0JckbgHur6vpx1zJCOwEvBc6uqgOBh5nBywU/i5rr4ssZhOBzgKcleet4qxq95sHbGbv3vy9B0Ka7izknyc4MQuD8qrp03PV07BXAsiTrGVz6e3WSvxlvSZ3bAGyoqokzvYsZBMNcdhhwR1VtrKpHgUuBXxlzTaPyvSTPBmh+3ztTG+5LELTp7mJOSRIG145vqaozx11P16rqD6tqz6paxODf98qqmtPfFKvqu8BdSZ7fzHoNj+/mfS66Ezg4yVOb/+OvYY43kA8Z7pLneOCymdrwrOh9dEdN1d3FmMvq2iuA3wRuTPKNZt4fVdWqMdakmfdu4PzmC8464O1jrqdTVfXVJBcDNzC4M+7rzMGuJpJcABwK7JFkA/AnwOnARUlOAL4DvGXG9mcXE5LUb325NCRJmoJBIEk9ZxBIUs8ZBJLUcwaBJPWcQaBZIUkl+cjQ9PuT/OkMbfuvkhw1E9vayn6ObnoIvWrS/EXDvUxO8d5Dt7VH1SRXJ+nV4O7aPgaBZoufAG9Osse4CxnWdHzW1gnA71TVq7qqR9oeBoFmi00MHhw6ZfKCyd/ok/yw+X1oki8luSzJuiSnJzkuydeS3Jhkv6HNHJZkTZLbmn6LJsY2+HCS1Um+meR3h7b7lSQr2cKTvEmObbb/rSRnNPNOBX4V+GSSD091kM3ZwVeS3ND8DHefsFuSy5txNf4yybzmPa9Lcm2z/meb/qWGtzm/+Rt9q6nrCX9D9VsvnizWnHEW8M0kH9qG9xwA/CKDLn3XAedW1UHNQD3vBt7brLeIQXfl+wFXJXke8FsMerd8eZInAdckmejp8qXAi6rqjuGdJXkOcAbwMuA+4J+SvKmqTkvyauD9VbVmmnrvBV5bVT9Oshi4AJi4vHMQg/E0vgN8nsEZ0tXAHwOHVdXDSf4AeB9w2tA2XwIsbPrvJ8kzWv3l1BsGgWaNqnowyWcYDEzy7y3ftnqi694k3wYmPshvBIYv0VxUVZuB25OsA14AvA548dDZxu7AYuAR4GuTQ6DxcuDqqtrY7PN8BmMG/EPLencGPprkJcBjwP5Dy75WVeua7V7A4AzjxwzC4ZpB1zvsAlw7aZvrgH2T/AVw+dDfQAIMAs0+f8agn5lPD83bRHOZs7lcssvQsp8Mvd48NL2Zx///n9zXSgEB3l1VVwwvSHIogy6fu3AK8D0GZzLzGHzQb63GL1TVsVNtsKruS3IA8HrgHQz6qPntmSxas5ttBJpVquoHwEUMGl4nrGdwKQZgGYNv1dvq6CTzmnaDfYFbGXRS+M6mO2+S7N9i4JevAYck2SODIVKPBb60DXXsDtzTnJ38JoNOEicc1PSgOw/4deCfgeuAVzSXskjytCTDZxE0DezzquoSBpeR5npX1dpGnhFoNvoIcNLQ9CeAy5L8C4Nr59vzbf1OBh/iuwHvaK7Rn8ug7eCGpsvjjWxleMCquifJCuAqBt/WL6+qbeku+GPAJUl+iycey2rgo8Dzmu3/fVVtTvI24IKmHQMGH/a3Db1vIYNRzCa++P3hNtSjHrD3UUnqOS8NSVLPGQSS1HMGgST1nEEgST1nEEhSzxkEktRzBoEk9dz/B1F2XUu4roQ6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_label_frequency(L):\n",
    "    plt.hist((L != ABSTAIN).sum(axis=1), density=True, bins=range(L.shape[1]))\n",
    "    plt.xlabel(\"Number of labels\")\n",
    "    plt.ylabel(\"Fraction of dataset\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_label_frequency(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "source": [
    "We see that over half of our `train` dataset data points have 2 or fewer labels from LFs.\n",
    "Fortunately, the labels we do have can be used to train a classifier over the comment text directly, allowing this final machine learning model to generalize beyond what our labeling functions labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is now to convert the labels from our LFs into a single _noise-aware_ probabilistic (or confidence-weighted) label per data point.\n",
    "A simple baseline for doing this is to take the majority vote on a per-data point basis: if more LFs voted SPAM than HAM, label it SPAM (and vice versa).\n",
    "We can test this with the\n",
    "[`MajorityLabelVoter` baseline model](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.model.baselines.MajorityLabelVoter.html#snorkel.labeling.model.baselines.MajorityLabelVoter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as we can see from the summary statistics of our LFs in the previous section, they have varying properties and should not be treated identically. In addition to having varied accuracies and coverages, LFs may be correlated, resulting in certain signals being overrepresented in a majority-vote-based model. To handle these issues appropriately, we will instead use a more sophisticated Snorkel `LabelModel` to combine the outputs of the LFs.\n",
    "\n",
    "This model will ultimately produce a single set of noise-aware training labels, which are probabilistic or confidence-weighted labels. We will then use these labels to train a classifier for our task. For more technical details of this overall approach, see our [NeurIPS 2016](https://arxiv.org/abs/1605.07723) and [AAAI 2019](https://arxiv.org/abs/1810.02840) papers. For more info on the API, see the [`LabelModel` documentation](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.model.label_model.LabelModel.html#snorkel.labeling.model.label_model.LabelModel).\n",
    "\n",
    "Note that no gold labels are used during the training process.\n",
    "The only information we need is the label matrix, which contains the output of the LFs on our training set.\n",
    "The `LabelModel` is able to learn weights for the labeling functions using only the label matrix as input.\n",
    "We also specify the `cardinality`, or number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy:   83.6%\n",
      "Label Model Accuracy:     86.8%\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_train = majority_model.predict(L=L_train)\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123)\n",
    "\n",
    "majority_acc = majority_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority vote model or more sophisticated `LabelModel` could in principle be used directly as a classifier if the outputs of our labeling functions were made available at test time.\n",
    "However, these models (i.e. these re-weighted combinations of our labeling function's votes) will abstain on the data points that our labeling functions don't cover (and additionally, may require slow or unavailable features to execute at test time).\n",
    "In the next section, we will instead use the outputs of the `LabelModel` as training labels to train a discriminative classifier **which can generalize beyond the labeling function outputs** to see if we can improve performance further.\n",
    "This classifier will also only need the text of the comment to make predictions, making it much more suitable for inference over unseen comments.\n",
    "For more information on the properties of the label model, see the [Snorkel documentation](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.model.label_model.LabelModel.html#snorkel.labeling.model.label_model.LabelModel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "source": [
    "Let's briefly confirm that the labels the `LabelModel` produces are indeed probabilistic in nature.\n",
    "The following histogram shows the confidences we have that each data point has the label SPAM.\n",
    "The points we are least certain about will have labels close to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def plot_probabilities_histogram(Y):\n",
    "    plt.hist(Y, bins=10)\n",
    "    plt.xlabel(\"Probability of SPAM\")\n",
    "    plt.ylabel(\"Number of data points\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "probs_train = label_model.predict_proba(L=L_train)\n",
    "plot_probabilities_histogram(probs_train[:, SPAM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out unlabeled data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw earlier, some of the data points in our `train` set received no labels from any of our LFs.\n",
    "These data points convey no supervision signal and tend to hurt performance, so we filter them out before training using a\n",
    "[built-in utility](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.filter_unlabeled_dataframe.html#snorkel.labeling.filter_unlabeled_dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1586, 5)\n",
      "(1320, 5)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "    X=df_train, y=probs_train, L=L_train\n",
    ")\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_train_filtered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section of the tutorial, we'll use the probabilistic training labels we generated in the last section to train a classifier for our task.\n",
    "**The output of the Snorkel `LabelModel` is just a set of labels which can be used with most popular libraries for performing supervised learning, such as TensorFlow, Keras, PyTorch, Scikit-Learn, Ludwig, and XGBoost.**\n",
    "In this tutorial, we use the well-known library [Scikit-Learn](https://scikit-learn.org).\n",
    "**Note that typically, Snorkel is used (and really shines!) with much more complex, training data-hungry models, but we will use Logistic Regression here for simplicity of exposition.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity and speed, we use a simple \"bag of n-grams\" feature representation: each data point is represented by a one-hot vector marking which words or 2-word combinations are present in the comment text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1320, 56734)\n",
      "(250, 56734)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 5))\n",
    "X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())\n",
    "X_test = vectorizer.transform(df_test.text.tolist())\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in Section 4, the `LabelModel` outputs probabilistic (float) labels.\n",
    "If the classifier we are training accepts target labels as floats, we can train on these labels directly (see describe the properties of this type of \"noise-aware\" loss in our [NeurIPS 2016 paper](https://arxiv.org/abs/1605.07723)).\n",
    "\n",
    "If we want to use a library or model that doesn't accept probabilistic labels (such as Scikit-Learn), we can instead replace each label distribution with the label of the class that has the maximum probability.\n",
    "This can easily be done using the\n",
    "[`probs_to_preds` helper method](https://snorkel.readthedocs.io/en/master/packages/_autosummary/utils/snorkel.utils.probs_to_preds.html#snorkel.utils.probs_to_preds).\n",
    "We do note, however, that this transformation is lossy, as we no longer have values for our confidence in each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "preds_train_filtered = probs_to_preds(probs=probs_train_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use these labels to train a classifier as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance for  LogisticRegression\n",
      "Test Accuracy: 78.0%\n",
      "Performance for  LinearSVC\n",
      "Test Accuracy: 80.4%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "for classifier in [LogisticRegression(C=1e3, solver=\"liblinear\"), LinearSVC()]:\n",
    "    classifier.fit(X=X_train, y=preds_train_filtered)\n",
    "    print(\"Performance for \", type(classifier).__name__), \n",
    "    print(f\"Test Accuracy: {classifier.score(X=X_test, y=Y_test) * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe an additional boost in accuracy over the `LabelModel` by multiple points! This is in part because the discriminative model generalizes beyond the labeling function's labels and makes good predictions on all data points, not just the ones covered by labeling functions.\n",
    "By using the label model to transfer the domain knowledge encoded in our LFs to the discriminative model,\n",
    "we were able to generalize beyond the noisy labeling heuristics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we accomplished the following:\n",
    "* We introduced the concept of Labeling Functions (LFs) and demonstrated some of the forms they can take.\n",
    "* We used the Snorkel `LabelModel` to automatically learn how to combine the outputs of our LFs into strong probabilistic labels.\n",
    "* We showed that a classifier trained on a weakly supervised dataset can outperform an approach based on the LFs alone as it learns to generalize beyond the noisy heuristics we provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation: Writing Transformation Functions (TFs)\n",
    "\n",
    "Transformation functions are functions that can be applied to a training data point to create another valid training data point of the same class.\n",
    "For example, for image classification problems, it is common to rotate or crop images in the training data to create new training inputs.\n",
    "Transformation functions should be atomic e.g. a small rotation of an image, or changing a single word in a sentence.\n",
    "We then compose multiple transformation functions when applying them to training data points.\n",
    "\n",
    "Common ways to augment text includes replacing words with their synonyms, or replacing names entities with other entities.\n",
    "More info can be found\n",
    "[here](https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28) or\n",
    "[here](https://towardsdatascience.com/these-are-the-easiest-data-augmentation-techniques-in-natural-language-processing-you-can-think-of-88e393fd610).\n",
    "Our basic modeling assumption is that applying these operations to a comment generally shouldn't change whether it is `SPAM` or not.\n",
    "\n",
    "Transformation functions in Snorkel are created with the\n",
    "[`transformation_function` decorator](https://snorkel.readthedocs.io/en/master/packages/_autosummary/augmentation/snorkel.augmentation.transformation_function.html#snorkel.augmentation.transformation_function),\n",
    "which wraps a function that takes in a single data point and returns a transformed version of the data point.\n",
    "If no transformation is possible, a TF can return `None` or the original data point.\n",
    "If all the TFs applied to a data point return `None`, the data point won't be included in\n",
    "the augmented dataset when we apply our TFs below.\n",
    "\n",
    "Just like the `labeling_function` decorator, the `transformation_function` decorator\n",
    "accepts `pre` argument for `Preprocessor` objects.\n",
    "Here, we'll use a\n",
    "[`SpacyPreprocessor`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/preprocess/snorkel.preprocess.nlp.SpacyPreprocessor.html#snorkel.preprocess.nlp.SpacyPreprocessor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us reload the dataset, with labels now. \n",
    "df_train, df_test = load_spam_dataset(load_train_labels=True)\n",
    "# We pull out the label vectors for ease of use later\n",
    "Y_train = df_train[\"label\"].values\n",
    "Y_test = df_test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)\n",
    "\n",
    "import names\n",
    "from snorkel.augmentation import transformation_function\n",
    "\n",
    "# Pregenerate some random person names to replace existing ones with\n",
    "# for the transformation strategies below\n",
    "replacement_names = [names.get_full_name() for _ in range(50)]\n",
    "\n",
    "\n",
    "# Replace a random named entity with a different entity of the same type.\n",
    "@transformation_function(pre=[spacy])\n",
    "def change_person(x):\n",
    "    person_names = [ent.text for ent in x.doc.ents if ent.label_ == \"PERSON\"]\n",
    "    # If there is at least one person name, replace a random one. Else return None.\n",
    "    if person_names:\n",
    "        name_to_replace = np.random.choice(person_names)\n",
    "        replacement_name = np.random.choice(replacement_names)\n",
    "        x.text = x.text.replace(name_to_replace, replacement_name)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Swap two adjectives at random.\n",
    "@transformation_function(pre=[spacy])\n",
    "def swap_adjectives(x):\n",
    "    adjective_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"ADJ\"]\n",
    "    # Check that there are at least two adjectives to swap.\n",
    "    if len(adjective_idxs) >= 2:\n",
    "        idx1, idx2 = sorted(np.random.choice(adjective_idxs, 2, replace=False))\n",
    "        # Swap tokens in positions idx1 and idx2.\n",
    "        x.text = \" \".join(\n",
    "            [\n",
    "                x.doc[:idx1].text,\n",
    "                x.doc[idx2].text,\n",
    "                x.doc[1 + idx1 : idx2].text,\n",
    "                x.doc[idx1].text,\n",
    "                x.doc[1 + idx2 :].text,\n",
    "            ]\n",
    "        )\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add other wordnet based transformation funtions from NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/bangaru/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "def get_synonym(word, pos=None):\n",
    "    \"\"\"Get synonym for word given its part-of-speech (pos).\"\"\"\n",
    "    synsets = wn.synsets(word, pos=pos)\n",
    "    # Return None if wordnet has no synsets (synonym sets) for this word and pos.\n",
    "    if synsets:\n",
    "        words = [lemma.name() for lemma in synsets[0].lemmas()]\n",
    "        if words[0].lower() != word.lower():  # Skip if synonym is same as word.\n",
    "            # Multi word synonyms in wordnet use '_' as a separator e.g. reckon_with. Replace it with space.\n",
    "            return words[0].replace(\"_\", \" \")\n",
    "\n",
    "\n",
    "def replace_token(spacy_doc, idx, replacement):\n",
    "    \"\"\"Replace token in position idx with replacement.\"\"\"\n",
    "    return \" \".join([spacy_doc[:idx].text, replacement, spacy_doc[1 + idx :].text])\n",
    "\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_verb_with_synonym(x):\n",
    "    # Get indices of verb tokens in sentence.\n",
    "    verb_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"VERB\"]\n",
    "    if verb_idxs:\n",
    "        # Pick random verb idx to replace.\n",
    "        idx = np.random.choice(verb_idxs)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=\"v\")\n",
    "        # If there's a valid verb synonym, replace it. Otherwise, return None.\n",
    "        if synonym:\n",
    "            x.text = replace_token(x.doc, idx, synonym)\n",
    "            return x\n",
    "\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_noun_with_synonym(x):\n",
    "    # Get indices of noun tokens in sentence.\n",
    "    noun_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"NOUN\"]\n",
    "    if noun_idxs:\n",
    "        # Pick random noun idx to replace.\n",
    "        idx = np.random.choice(noun_idxs)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=\"n\")\n",
    "        # If there's a valid noun synonym, replace it. Otherwise, return None.\n",
    "        if synonym:\n",
    "            x.text = replace_token(x.doc, idx, synonym)\n",
    "            return x\n",
    "\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_adjective_with_synonym(x):\n",
    "    # Get indices of adjective tokens in sentence.\n",
    "    adjective_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"ADJ\"]\n",
    "    if adjective_idxs:\n",
    "        # Pick random adjective idx to replace.\n",
    "        idx = np.random.choice(adjective_idxs)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=\"a\")\n",
    "        # If there's a valid adjective synonym, replace it. Otherwise, return None.\n",
    "        if synonym:\n",
    "            x.text = replace_token(x.doc, idx, synonym)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF Name</th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Transformed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>change_person</td>\n",
       "      <td>Check out Berzerk video on my channel ! :D</td>\n",
       "      <td>Check out Timothy Seymour video on my channel ! :D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>swap_adjectives</td>\n",
       "      <td>hey guys look im aware im spamming and it pisses people off but please take a moment to check out my music.  im a young rapper and i love to do it and i just wanna share my music with more people  just click my picture and then see if you like my stuff</td>\n",
       "      <td>hey guys look im young im spamming and it pisses people off but please take a moment to check out my music.  im a aware rapper and i love to do it and i just wanna share my music with more people  just click my picture and then see if you like my stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>replace_verb_with_synonym</td>\n",
       "      <td>\"eye of the tiger\" \"i am the champion\" seems like katy perry is using  titles of old rock songs for lyrics..﻿</td>\n",
       "      <td>\"eye of the tiger\" \"i be the champion\" seems like katy perry is using  titles of old rock songs for lyrics..﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>replace_noun_with_synonym</td>\n",
       "      <td>\"eye of the tiger\" \"i am the champion\" seems like katy perry is using  titles of old rock songs for lyrics..﻿</td>\n",
       "      <td>\"eye of the tiger\" \"i am the champion\" seems like katy perry is using  titles of old rock song for lyrics..﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>replace_adjective_with_synonym</td>\n",
       "      <td>I started hating Katy Perry after finding out that she stole all of the  ideas on her videos  from an old comic book. Yet, her music is catchy. ﻿</td>\n",
       "      <td>I started hating Katy Perry after finding out that she stole all of the  ideas on her videos  from an old amusing book. Yet, her music is catchy. ﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          TF Name  \\\n",
       "0  change_person                    \n",
       "1  swap_adjectives                  \n",
       "2  replace_verb_with_synonym        \n",
       "3  replace_noun_with_synonym        \n",
       "4  replace_adjective_with_synonym   \n",
       "\n",
       "                                                                                                                                                                                                                                                  Original Text  \\\n",
       "0  Check out Berzerk video on my channel ! :D                                                                                                                                                                                                                     \n",
       "1  hey guys look im aware im spamming and it pisses people off but please take a moment to check out my music.  im a young rapper and i love to do it and i just wanna share my music with more people  just click my picture and then see if you like my stuff   \n",
       "2  \"eye of the tiger\" \"i am the champion\" seems like katy perry is using  titles of old rock songs for lyrics..﻿                                                                                                                                                  \n",
       "3  \"eye of the tiger\" \"i am the champion\" seems like katy perry is using  titles of old rock songs for lyrics..﻿                                                                                                                                                  \n",
       "4  I started hating Katy Perry after finding out that she stole all of the  ideas on her videos  from an old comic book. Yet, her music is catchy. ﻿                                                                                                              \n",
       "\n",
       "                                                                                                                                                                                                                                               Transformed Text  \n",
       "0  Check out Timothy Seymour video on my channel ! :D                                                                                                                                                                                                            \n",
       "1  hey guys look im young im spamming and it pisses people off but please take a moment to check out my music.  im a aware rapper and i love to do it and i just wanna share my music with more people  just click my picture and then see if you like my stuff  \n",
       "2  \"eye of the tiger\" \"i be the champion\" seems like katy perry is using  titles of old rock songs for lyrics..﻿                                                                                                                                                 \n",
       "3  \"eye of the tiger\" \"i am the champion\" seems like katy perry is using  titles of old rock song for lyrics..﻿                                                                                                                                                  \n",
       "4  I started hating Katy Perry after finding out that she stole all of the  ideas on her videos  from an old amusing book. Yet, her music is catchy. ﻿                                                                                                           "
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs = [\n",
    "    change_person,\n",
    "    swap_adjectives,\n",
    "    replace_verb_with_synonym,\n",
    "    replace_noun_with_synonym,\n",
    "    replace_adjective_with_synonym,\n",
    "]\n",
    "from utils import preview_tfs\n",
    "import numpy as np\n",
    "\n",
    "preview_tfs(df_train, tfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a couple of things about the TFs.\n",
    "\n",
    "    Sometimes they make trivial changes (\"website\" to \"web site\" for replace_noun_with_synonym). This can still be helpful for training our model, because it teaches the model to be invariant to such small changes.\n",
    "    Sometimes they introduce incorrect grammar to the sentence (e.g. swap_adjectives swapping \"young\" and \"more\" above).\n",
    "\n",
    "The TFs are expected to be heuristic strategies that indeed preserve the class most of the time, but don't need to be perfect. This is especially true when using automated data augmentation techniques which can learn to avoid particularly corrupted data points. As we'll see below, Snorkel is compatible with such learned augmentation policies.\n",
    "\n",
    "3. Applying Transformation Functions\n",
    "\n",
    "We'll first define a Policy to determine what sequence of TFs to apply to each data point. We'll start with a RandomPolicy that samples sequence_length=2 TFs to apply uniformly at random per data point. The n_per_original argument determines how many augmented data points to generate per original data point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.augmentation import RandomPolicy\n",
    "\n",
    "random_policy = RandomPolicy(\n",
    "    len(tfs), sequence_length=2, n_per_original=2, keep_original=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we can do better than uniform random sampling. We might have domain knowledge that some TFs should be applied more frequently than others, or have trained an automated data augmentation model that learned a sampling distribution for the TFs. Snorkel supports this use case with a MeanFieldPolicy, which allows you to specify a sampling distribution for the TFs. We give higher probabilities to the replace_[X]_with_synonym TFs, since those provide more information to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.augmentation import MeanFieldPolicy\n",
    "\n",
    "mean_field_policy = MeanFieldPolicy(\n",
    "    len(tfs),\n",
    "    sequence_length=2,\n",
    "    n_per_original=2,\n",
    "    keep_original=True,\n",
    "    p=[0.05, 0.05, 0.3, 0.3, 0.3],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1586/1586 [00:28<00:00, 56.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1586, 5)\n",
      "(2543, 5)\n"
     ]
    }
   ],
   "source": [
    "#To apply one or more TFs that we've written to a collection of data points according to our policy, \n",
    "#we use a PandasTFApplier because our data points are represented with a Pandas DataFrame.\n",
    "\n",
    "from snorkel.augmentation import PandasTFApplier\n",
    "\n",
    "tf_applier = PandasTFApplier(tfs, mean_field_policy)\n",
    "\n",
    "df_train_augmented = tf_applier.apply(df_train)\n",
    "\n",
    "Y_train_augmented = df_train_augmented[\"label\"].values\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_train_augmented.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have almost doubled our dataset using TFs! Note that despite n_per_original being set to 2, our dataset may not exactly triple in size, because sometimes TFs return None instead of a new data point (e.g. change_person when applied to a sentence with no persons). If you prefer to have exact proportions for your dataset, you can have TFs that can't perform a valid transformation return the original data point rather than None (as they do here).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training A Model\n",
    "\n",
    "Our final step is to use the augmented data to train a model. We can use the same classifier setup as before:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2543, 73338)\n",
      "(250, 73338)\n",
      "(2543,)\n",
      "{0, 1}\n",
      "Performance for  LogisticRegression\n",
      "Test Accuracy: 88.4%\n",
      "Performance for  LinearSVC\n",
      "Test Accuracy: 92.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 5))\n",
    "X_train_augmented = vectorizer.fit_transform(df_train_augmented.text.tolist())\n",
    "\n",
    "X_test = vectorizer.transform(df_test.text.tolist())\n",
    "\n",
    "print(X_train_augmented.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train_augmented.shape)\n",
    "print(set(Y_train_augmented))\n",
    "\n",
    "for classifier in [LogisticRegression(C=1e3, solver=\"liblinear\"), LinearSVC()]:\n",
    "    classifier.fit(X=X_train_augmented, y=Y_train_augmented)\n",
    "    print(\"Performance for \", type(classifier).__name__), \n",
    "    print(f\"Test Accuracy: {classifier.score(X=X_test, y=Y_test) * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1586, 64101)\n",
      "(250, 64101)\n",
      "(1586,)\n",
      "Performance for  LogisticRegression\n",
      "Test Accuracy: 88.4%\n",
      "Performance for  LinearSVC\n",
      "Test Accuracy: 93.2%\n"
     ]
    }
   ],
   "source": [
    "#Performance of the dataset, with known labels, with some basic feature engineering\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 5))\n",
    "X_train = vectorizer.fit_transform(df_train.text.tolist())\n",
    "X_test = vectorizer.transform(df_test.text.tolist())\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "for classifier in [LogisticRegression(C=1e3, solver=\"liblinear\"), LinearSVC()]:\n",
    "    classifier.fit(X=X_train, y=Y_train)\n",
    "    print(\"Performance for \", type(classifier).__name__), \n",
    "    print(f\"Test Accuracy: {classifier.score(X=X_test, y=Y_test) * 100:.1f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, was this a useful exercise?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
